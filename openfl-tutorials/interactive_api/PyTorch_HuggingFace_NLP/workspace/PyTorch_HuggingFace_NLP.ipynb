{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated fine-tuning of a language model using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"datasets==3.0.0\" \"transformers==4.44.2\" \"evaluate==0.4.3\" \"ipywidgets==8.1.5\" \"torch==2.4.1\" \"accelerate==0.34.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the federated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "federation = Federation(\n",
    "    client_id=\"frontend\",\n",
    "    director_node_fqdn=\"localhost\",\n",
    "    director_port=50050,\n",
    "    tls=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define federated experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import FLExperiment\n",
    "\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=\"nlp_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import ModelInterface\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, AutoTokenizer, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=model.config.max_position_embeddings)\n",
    "\n",
    "for param in model.parameters(): param.data = param.data.contiguous()\n",
    "params_to_update = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(params_to_update, lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "framework_adapter = \"openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin\"\n",
    "MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define federated tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuned_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    seed=12345,\n",
    "    use_cpu=True,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    log_level=\"debug\",\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(\n",
    "        predictions=predictions, references=labels\n",
    "    )[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )[\"f1\"]\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "TI = TaskInterface()\n",
    "\n",
    "@TI.register_fl_task(\n",
    "    model=\"model\", data_loader=\"train_loader\", device=\"device\", optimizer=\"optimizer\",\n",
    ")\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    tokenized_dataset = train_loader.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        optimizers=(optimizer, None),\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer.train().metrics\n",
    "\n",
    "@TI.register_fl_task(\n",
    "    model=\"model\", data_loader=\"test_loader\", device=\"device\",\n",
    ")\n",
    "def validate(model, test_loader, device):\n",
    "    tokenized_dataset = test_loader.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import DataInterface\n",
    "\n",
    "class CustomDataLoader(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        self.train_set = self._shard_descriptor.get_dataset(\"train\")\n",
    "        self.test_set = self._shard_descriptor.get_dataset(\"test\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.shard_descriptor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shard_descriptor)\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        return self.train_set\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        return self.test_set\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        return len(self.test_set)\n",
    "\n",
    "data_loader = CustomDataLoader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.start(\n",
    "    model_provider=MI,\n",
    "    task_keeper=TI,\n",
    "    data_loader=data_loader,\n",
    "    rounds_to_train=3,\n",
    "    opt_treatment=\"CONTINUE_GLOBAL\",\n",
    ")\n",
    "\n",
    "fl_experiment.stream_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testfl_expr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
